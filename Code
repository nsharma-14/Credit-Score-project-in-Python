#Installing required libraries
pip install category_encoders
pip install sklearn

#Importing required libraries
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import warnings, re, joblib
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder, PowerTransformer, FunctionTransformer, OneHotEncoder, LabelEncoder
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris
from sklearn.metrics import cohen_kappa_score, make_scorer
from sklearn.naive_bayes import GaussianNB
from sklearn import tree
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import roc_auc_score
import category_encoders as ce

#Reading the data
data = pd.read_csv("/Users/natasha/Desktop/Ualbany Courses/Fall Sem Courses 2023/CINF 528/Credit Scoring Project - 528/Credit Scoring Project/train.csv" , sep = "," ,na_values =['_','_______','#F%$D@*&8','!@9#%8','nan'])
data.head()

#Data Description
1. ID: An identifier for each record in the dataset.
2. Customer_ID: Identifier for individual customers.
3. Month: The month associated with the data entry.
4. Name: Customer's name.
5. Age: Customer's age.
6. SSN: Social Security Number or some other form of identification.
7. Occupation: Customer's occupation or job title.
8. Annual_Income: Customer's annual income.
9. Monthly_Inhand_Salary: The amount of money the customer receives as salary on a monthly basis.
10. Num_Bank_Accounts: Number of bank accounts the customer has.
11. Num_Credit_Card: Number of credit cards the customer possesses.
12. Interest_Rate: The interest rate associated with some financial aspect (e.g., loans or credit cards).
13. Num_of_Loan: Number of loans the customer has.
14. Type_of_Loan: The type of loan(s) the customer has (e.g., mortgage, personal loan, etc.).
15. Delay_from_due_date: Delay in payments from the due date.
16. Num_of_Delayed_Payment: Number of delayed payments.
17. Changed_Credit_Limit: Indicates if the customer has changed their credit limit.
18. Num_Credit_Inquiries: Number of credit inquiries made by the customer.
19. Credit_Mix: The mix of different types of credit accounts (e.g., credit cards, loans).
20. Outstanding_Debt: The amount of outstanding debt.
21. Credit_Utilization_Ratio: The ratio of credit used to the total credit limit.
22. Credit_History_Age: The age of the customer's credit history.
23. Payment_of_Min_Amount: Payment behavior regarding minimum required payments.
24. Total_EMI_per_month: Total Equated Monthly Installment (EMI) payments made by the customer.
25. Amount_invested_monthly: The amount the customer invests on a monthly basis.
26. Payment_Behaviour: Behavior related to payments, possibly indicating patterns or trends.
27. Monthly_Balance: The customer's monthly balance in their financial accounts.
28. Credit_Score: The credit score associated with the customer's creditworthiness.

#Data Exploratory Analysis

#Checking the head of the data
data.head()

#Checking the shape of the data
rows, cols = data.shape
print("Rows: " + str(rows) + "\nColumns: " + str(cols) )

#Checking the datatypes
data.dtypes

#Checking if there is any null value in the data
data.isnull().sum()

#Describing the data
data.describe()

#Data Wrangling/Data Preprocessing/Data Cleaning


## Changing datatypes:

## Month to to_datetime
## Age and Num_of_Loan to Integer 
## Num_of_Delayed_Payment, Annual_Income, Outstanding_Debt , Amount_invested_monthly and Monthly_Balance to Float
## Payment_of_Min_Amount to Binary


data["Month"] = pd.to_datetime(data['Month'], format='%B').dt.month

data["Age"] = data["Age"].str.extract("(\d+)").astype(int)

data["Num_of_Loan"] = data["Num_of_Loan"].str.extract("(\d+)").astype(int)

data["Num_of_Delayed_Payment"] = data["Num_of_Delayed_Payment"].str.extract("(\d+)").astype(float)

data["Annual_Income"] = data["Annual_Income"].str.extract(r'(\d+\.\d+)').astype(float)

data["Outstanding_Debt"] = data["Outstanding_Debt"].str.extract(r'(\d+\.\d+)').astype(float)

data["Amount_invested_monthly"] = data["Amount_invested_monthly"].str.extract(r'(\d+\.\d+)').astype(float)

data["Monthly_Balance"] = data["Monthly_Balance"].str.extract(r'(\d+\.\d+)').astype(float)

data['Payment_of_Min_Amount'] = data['Payment_of_Min_Amount'].map( 
                   {'yes':True ,'no':False}) 

data['Payment_of_Min_Amount'] = data['Payment_of_Min_Amount'].astype(bool)


#Removing Unwanted columns
data = data.drop(columns=["Name","SSN"])

#Checking the columns
data.columns

#Again, describing the data
data.describe()


#More Cleaning

#Age column
#### Looks like there are some random values in the Age column since mean is 119.5 and max is 8698

def clean_age(group):
    min_age = group['Age'].min()
    max_age = group['Age'].max()
    group['Age'] = group['Age'].apply(lambda x: min_age if x < 14 or x > 70 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(clean_age).reset_index(drop=True)

#Occupation Column
data[data['Occupation'].isnull()]

#There are multiple rows in the Occupation column which are null. However since there is a repetition of the customers, 
#we can use one of the rows which do have Occupation to fill the other rows for the same customer.

all_occupations = data.dropna(subset=['Occupation']).set_index('Customer_ID')['Occupation'].to_dict()
data['Occupation'] = data.apply(lambda row: all_occupations.get(row['Customer_ID'], row['Occupation']), axis=1)

### Type of Loan
data[data['Type_of_Loan'].isnull()]
data['Type_of_Loan'].fillna('NA', inplace=True)

#Changed_Credit_Limit
data[data['Changed_Credit_Limit'].isnull()]

data['Changed_Credit_Limit'] = data.groupby('Customer_ID')['Changed_Credit_Limit'].transform(lambda x: x.interpolate(method='index', limit_direction='both'))

data[data['Changed_Credit_Limit'].isnull()]

#Num_Credit_Inquiries
data[data['Num_Credit_Inquiries'].isnull()]

customer_ids = data[(data['Num_Credit_Inquiries'].isna())]['Customer_ID'].values

# Group by 'Customer_ID' and perform index interpolation
data['Num_Credit_Inquiries'] = data.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(
    lambda x: x.interpolate(method='index', limit_direction='both') if x.count() > 1 else x)


customer_ids = data[(data['Num_Credit_Inquiries'] > 15) ]['Customer_ID'].values

for id in customer_ids:

    mode_v = data[data['Customer_ID'] == id]['Num_Credit_Inquiries'].mode()[0]

    data.loc[(data['Customer_ID'] == id) &
                ((data['Num_Credit_Inquiries'] > 15) ),
                ['Num_Credit_Inquiries']]=mode_v


#Credit Mix

# Create a dictionary to map Customer_ID to non-null Credit_Mix values
Credit_Mix_mapping = data.dropna(subset=['Credit_Mix']).set_index('Customer_ID')['Credit_Mix'].to_dict()

# Fill missing Credit_Mix values based on Customer_ID
data['Credit_Mix'] = data.apply(lambda row: Credit_Mix_mapping.get(row['Customer_ID'], row['Credit_Mix']), axis=1)

#Num_of_Delayed_Payment

customer_ids = data[(data['Num_of_Delayed_Payment'].isna())]['Customer_ID'].values

# Group by 'Customer_ID' and perform index interpolation
data['Num_of_Delayed_Payment'] = data.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(
    lambda x: x.interpolate(method='index', limit_direction='both') if x.count() > 1 else x)

#Monthly_Inhand_Salary

# Create a dictionary to map Customer_ID to non-null Monthly_Inhand_Salary values
Monthly_Inhand_Salary_mapping = data.dropna(subset=['Monthly_Inhand_Salary']).set_index('Customer_ID')['Monthly_Inhand_Salary'].to_dict()

# Fill missing Monthly_Inhand_Salary values based on Customer_ID
data['Monthly_Inhand_Salary'] = data.apply(lambda row: Monthly_Inhand_Salary_mapping.get(row['Customer_ID'], row['Monthly_Inhand_Salary']), axis=1)

customer_ids = data[(data['Amount_invested_monthly'].isna())]['Customer_ID'].values

# Group by 'Customer_ID' and perform index interpolation
data['Amount_invested_monthly'] = data.groupby('Customer_ID')['Amount_invested_monthly'].transform(
    lambda x: x.interpolate(method='index', limit_direction='both') if x.count() > 1 else x)

#Payment Behaviour

# Define a custom function to calculate mode with handling for NaN values
def custom_mode(series):
    mode_values = series.dropna().mode()
    if mode_values.empty:
        return np.nan
    return mode_values.iloc[0]

# Calculate the mode payment behavior for each customer
customer_mode_payment = data.groupby('Customer_ID')['Payment_Behaviour'].transform(custom_mode)

# Fill missing values with the corresponding customer's mode payment behavior
data['Payment_Behaviour'] = data['Payment_Behaviour'].fillna(customer_mode_payment)

#Monthly Balance

customer_ids = data[(data['Monthly_Balance'].isna())]['Customer_ID'].values

# Group by 'Customer_ID' and perform index interpolation
data['Monthly_Balance'] = data.groupby('Customer_ID')['Monthly_Balance'].transform(
    lambda x: x.interpolate(method='index', limit_direction='both') if x.count() > 1 else x)


selected_columns = data.select_dtypes(include=['number'])
# selected_columns = data.select_dtypes(include=['number'])


correlation_matrix = selected_columns.corr()


plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Factors vs. Monthly Balance')
plt.show()


imputer = SimpleImputer(strategy='most_frequent')  

data['Monthly_Balance'] = imputer.fit_transform(data[['Monthly_Balance']])

customer_mode_payment = data.groupby('Monthly_Inhand_Salary')['Monthly_Balance'].transform(custom_mode)

# Fill missing values with the corresponding customer's mode payment behavior
data['Monthly_Balance'] = data['Monthly_Balance'].fillna(customer_mode_payment)

## Credit History Age

def Month_Converter(x):
    if pd.notnull(x):
        num1 = int(x.split(' ')[0])
        num2 = int(x.split(' ')[3])
      
        return (num1*12)+num2
    else:
        return x


data.Credit_History_Age = data.Credit_History_Age.apply(lambda x: Month_Converter(x)).astype(np.float64)

customer_ids = data[(data['Credit_History_Age'].isna())]['Customer_ID'].values

# Group by 'Customer_ID' and perform index interpolation
data['Credit_History_Age'] = data.groupby('Customer_ID')['Credit_History_Age'].transform(
    lambda x: x.interpolate(method='index', limit_direction='both') if x.count() > 1 else x)

data["Credit_History_Age"]

##Removing Outliers

num_vars = data.select_dtypes(include=['number'])
sns.set(style='whitegrid')  
fig, axes = plt.subplots(5, 4, figsize=(18, 18), sharey=True) 

axes = axes.flatten()

for i, feature in enumerate(num_vars.columns):
    sns.violinplot(x=data[feature], ax=axes[i], color=np.random.rand(3,))  
    axes[i].set_title(f'Feature {feature}')  


plt.tight_layout()

plt.show()


data['Age'].describe()

##Interest_Rate

data['Interest_Rate'].describe()

#Looks like there are some impossible values since the max is 5797% which is not possible. Hence we will have to replace these values

def replace_Interest_Rate(group):
    min_Interest_Rate = group['Interest_Rate'].min()
    max_Interest_Rate = group['Interest_Rate'].max()
    group['Interest_Rate'] = group['Interest_Rate'].apply(lambda x: min_Interest_Rate if x < 1 or x > 40 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(replace_Interest_Rate).reset_index(drop=True)

data['Interest_Rate'].describe()

##Annual_Income

data['Annual_Income'].describe()

#Looks like the max annual income is at 24 million. Looking at who this belongs to and if this customer occurs multiple times in the data.

max(data['Annual_Income'])
data[data['Annual_Income'] == max(data['Annual_Income'])]
data[data['Customer_ID'] == 'CUS_0xcfc']

#It looks like 24198062.0 may be incorrect for this customer. There could be multiple instances of this, which needs to be fixed.

def replace_Annual_Income(group):
    min_Annual_Income = group['Annual_Income'].min()
    max_Annual_Income = group['Annual_Income'].max()
    group['Annual_Income'] = group['Annual_Income'].apply(lambda x: min_Annual_Income if  x > 0.2e+06 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(replace_Annual_Income).reset_index(drop=True)

data['Annual_Income'].describe()


##Num_Credit_Card

data['Num_Credit_Card'].describe()

#1499 credit cards are not possible. This suggests the same problem as Annual Income

def replace_Num_Credit_Card(group):
    min_Num_Credit_Card = group['Num_Credit_Card'].min()
    max_Num_Credit_Card = group['Num_Credit_Card'].max()
    group['Num_Credit_Card'] = group['Num_Credit_Card'].apply(lambda x: min_Num_Credit_Card if x <1 or x > 15 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(replace_Num_Credit_Card).reset_index(drop=True)

data['Num_Credit_Card'].describe()

##Num_of_Delayed_Payment

data['Num_of_Delayed_Payment'].describe()

#We will use the same method to replace the abnormally high delayed payments

def replace_Num_of_Delayed_Payment(group):
    min_Num_of_Delayed_Payment = group['Num_of_Delayed_Payment'].min()
    max_Num_of_Delayed_Payment = group['Num_of_Delayed_Payment'].max()
    group['Num_of_Delayed_Payment'] = group['Num_of_Delayed_Payment'].apply(lambda x: min_Num_of_Delayed_Payment if x <1 or x > 40 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(replace_Num_of_Delayed_Payment).reset_index(drop=True)

data['Num_of_Delayed_Payment'].describe()


## Num of Bank Accounts

data['Num_Bank_Accounts'].describe()

data['Num_Bank_Accounts'] = data['Num_Bank_Accounts'].abs()
def replace_Num_Bank_Accounts(group):
    min_Num_Bank_Accounts = group['Num_Bank_Accounts'].min()
    max_Num_Bank_Accounts = group['Num_Bank_Accounts'].max()
    group['Num_Bank_Accounts'] = group['Num_Bank_Accounts'].apply(lambda x: min_Num_Bank_Accounts if x <1 or x > 20 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(replace_Num_Bank_Accounts).reset_index(drop=True)

data['Num_Bank_Accounts'].value_counts()

data['Num_Bank_Accounts'].describe()

## Num of Loan

data[data['Num_of_Loan']>10]

data['Num_of_Loan'].describe()

def replace_Num_of_Loan(group):
    min_Num_of_Loan = group['Num_of_Loan'].min()
    max_Num_of_Loan = group['Num_of_Loan'].max()
    group['Num_of_Loan'] = group['Num_of_Loan'].apply(lambda x: min_Num_of_Loan if x <1 or x > 10 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(replace_Num_of_Loan).reset_index(drop=True)

data['Num_of_Loan'].describe()

##Total EMI per Month

data['Total_EMI_per_month'].describe()

def replace_Total_EMI_per_month(group):
    min_Total_EMI_per_month = group['Total_EMI_per_month'].min()
    max_Total_EMI_per_month = group['Total_EMI_per_month'].max()
    group['Total_EMI_per_month'] = group['Total_EMI_per_month'].apply(lambda x: min_Total_EMI_per_month if x <1 or x > 400 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(replace_Total_EMI_per_month).reset_index(drop=True)

data['Total_EMI_per_month'].describe()


##Amount Invested Monthly

data['Amount_invested_monthly'].describe()

data[data['Amount_invested_monthly'] == max(data['Amount_invested_monthly'])]

data[data['Customer_ID'] == 'CUS_0x2617']

def replace_Amount_invested_monthly(group):
    min_Amount_invested_monthly = group['Amount_invested_monthly'].min()
    max_Amount_invested_monthly = group['Amount_invested_monthly'].max()
    group['Amount_invested_monthly'] = group['Amount_invested_monthly'].apply(lambda x: min_Amount_invested_monthly if x <1 or x > 800 else x)
    return group

# Apply the function to the DataFrame using groupby and transform
data = data.groupby('Customer_ID').apply(replace_Amount_invested_monthly).reset_index(drop=True)

data['Amount_invested_monthly'].describe()

##Final Violin Plot after removing outliers

num_vars = data.select_dtypes(include=['number'])
sns.set(style='whitegrid')  
fig, axes = plt.subplots(5, 4, figsize=(18, 18), sharey=True) 

axes = axes.flatten()

for i, feature in enumerate(num_vars.columns):
    sns.violinplot(x=data[feature], ax=axes[i], color=np.random.rand(3,))  
    axes[i].set_title(f'Feature {feature}')  


plt.tight_layout()

plt.show()



###Visualization

CrossTabResult = pd.crosstab(index=data['Payment_Behaviour'], columns=data['Credit_Score'])

print(CrossTabResult)
CrossTabResult.plot.barh()


CrossTabResult = pd.crosstab(index=data['Occupation'], columns=data['Credit_Score'])

print(CrossTabResult)
CrossTabResult.plot.bar(figsize=(20,10), rot=0)


sns.boxplot(x='Credit_Score', y='Age', data=data)
plt.show()

data_backup = data.copy()
data_backup
data = data_backup.copy()
data

############################################ 
############# Encoding #####################

#Converting the Credit Score column in numerical

columns=[
      'Credit_Score']
for item in columns:
    data[item] = LabelEncoder().fit_transform(data[item])

columns = ['Payment_of_Min_Amount','Payment_Behaviour','Occupation','Credit_Mix', 'Type_of_Loan']
for item in columns:
    data[item] = ce.LeaveOneOutEncoder().fit_transform(data[item],data['Credit_Score'])
    
columns2 =['Age','Num_Credit_Card','Interest_Rate','Delay_from_due_date','Num_of_Delayed_Payment','Monthly_Balance',
                           'Num_Credit_Inquiries','Changed_Credit_Limit','Credit_History_Age',
                           'Total_EMI_per_month']
for item in columns2:
    mean_encoding = data.groupby([item])['Credit_Score'].mean()
    data[item] = data[item].map(mean_encoding)


X=data.drop('Credit_Score',axis=1)
y = data.Credit_Score

X.drop(['Customer_ID','ID'],axis=1,inplace=True)


####Modelling

from sklearn.metrics import ConfusionMatrixDisplay, classification_report, precision_score, recall_score, accuracy_score, f1_score
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,shuffle=True,random_state=42)

model_names = []
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
auc_scores = []
specificity_scores = []

def cross_validation(X, y, model):
    scores = cross_val_score(model, X, y, cv=5)
    return (scores.mean(), scores.std() * 2)


def score_calculator(X_train,y_train,X_test,y_test,model):
    model.fit(X_train,y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    test_acc = accuracy_score(y_test, y_pred_test)
    print(classification_report(y_test,y_pred_test))
    precision = precision_score(y_test,y_pred_test,average='micro')
    recall = recall_score(y_test,y_pred_test,average='micro')
    specificity = recall_score(y_test, y_pred_test, average='micro', pos_label=0)
    f1 = f1_score(y_test,y_pred_test,average='micro')
    y_pred_prob_test = model.predict_proba(X_test)
    auc = roc_auc_score(y_test, y_pred_prob_test, average='macro', multi_class='ovr')
    ConfusionMatrixDisplay.from_predictions(y_test,y_pred_test)
    plt.show()
    return (y_pred_test, test_acc, precision, recall, auc, f1, specificity)


###Linear Discriminant Analysis Model

####### First, we need to perform CV on the solver parameter in the LinearDiscriminantAnalysis Model

solvers = ['svd', 'lsqr']
for solver in solvers:
    mean_acc, std_acc = cross_validation(X_train, y_train, LinearDiscriminantAnalysis(solver=solver))
    print("Accuracy: %0.3f (+/- %0.3f)" % (mean_acc, std_acc))

#The accuracy score for both the models is the same so we can use either of them. Taking solver as lsqr lets perform prediction

y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train,y_train,X_test,y_test,LinearDiscriminantAnalysis(solver="lsqr"))
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('LinearDiscriminantAnalysis')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


###GausianNB Model

y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train,y_train,X_test,y_test,GaussianNB())
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('GaussianNB')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


##Decision Tree Model

#### First we need to perform CV on the max_depth parameter in Decision Tree Model

max_depths = range(1,20)
for max_depth in max_depths:
    mean_acc, std_acc = cross_validation(X_train, y_train, tree.DecisionTreeClassifier(max_depth=max_depth))
    print("Max Depth: %0.1f /nAccuracy: %0.3f (+/- %0.3f)" % (max_depth, mean_acc, std_acc))

y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train,y_train,X_test,y_test,tree.DecisionTreeClassifier())
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('DecisionTreeClassifier')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


###Logistic Regression

scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)

scaler = StandardScaler().fit(X_test)
X_test_scaled = scaler.transform(X_test)

mean_acc, std_acc = cross_validation(X_train_scaled, y_train, LogisticRegression(multi_class='multinomial'))

y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train_scaled,y_train,X_test_scaled,y_test,LogisticRegression(max_iter = 200, multi_class='multinomial'))
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('LogisticRegression')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


####### K-Nearest Neighbour (k-NN)

mean_acc, std_acc = cross_validation(X_train_scaled, y_train, KNeighborsClassifier(n_neighbors=5))
print("Accuracy: %0.3f (+/- %0.3f)" % (mean_acc, std_acc))

y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train_scaled,y_train,X_test_scaled,y_test,KNeighborsClassifier(n_neighbors=5))
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('KNeighborsClassifier')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


###Artificial Neural Networks 

model = MLPClassifier(hidden_layer_sizes=(100, ), learning_rate_init=0.001,alpha=1, momentum=0.9,max_iter=1000)
mean_acc, std_acc = cross_validation(X_train_scaled, y_train, model)
print("Accuracy: %0.3f (+/- %0.3f)" % (mean_acc, std_acc))

y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train_scaled,y_train,X_test_scaled,y_test,model)
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('MLPClassifier')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)

###SVM

model = svm.SVC(kernel='linear', C=50, probability=True)
# mean_acc, std_acc = cross_validation(X_train_scaled, y_train, model)
# print("Accuracy: %0.3f (+/- %0.3f)" % (mean_acc, std_acc))

y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train_scaled,y_train,X_test_scaled,y_test,model)
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('SVC')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


###Random Forest

model = RandomForestClassifier(n_estimators=200)
mean_acc, std_acc = cross_validation(X_train_scaled, y_train, model)
print("Accuracy: %0.3f (+/- %0.3f)" % (mean_acc, std_acc))

y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train_scaled,y_train,X_test_scaled,y_test,model)
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('RandomForestClassifier')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


###Bagging

model = BaggingClassifier(tree.DecisionTreeClassifier(),max_samples=0.5, max_features=0.5)
mean_acc, std_acc = cross_validation(X_train_scaled, y_train, model)
print("Accuracy: %0.3f (+/- %0.3f)" % (mean_acc, std_acc))


y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train_scaled,y_train,X_test_scaled,y_test,model)
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('BaggingClassifier')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


####Boosting

model = AdaBoostClassifier(tree.DecisionTreeClassifier(),n_estimators=10,algorithm='SAMME',learning_rate=0.5)
mean_acc, std_acc = cross_validation(X_train_scaled, y_train, model)
print("Accuracy: %0.3f (+/- %0.3f)" % (mean_acc, std_acc))


y_pred_test, accuracy, precision, recall, auc, f1, specificity = score_calculator(X_train_scaled,y_train,X_test_scaled,y_test,model)
# print("Accuracy: %0.3f \nPrecision: %0.3f \nRecall: %0.3f \nf1_score: %0.3f \nspecificity: %0.3f \nAuc: %0.3f" % (accuracy, precision, recall, f1, specificity, auc))
model_names.append('AdaBoostClassifier')
accuracy_scores.append(accuracy)
precision_scores.append(precision)
recall_scores.append(recall)
f1_scores.append(f1)
specificity_scores.append(specificity)
auc_scores.append(auc)


###Results

model_report = pd.DataFrame({'model_names': model_names, 'accuracy_scores': accuracy_scores, 'precision_scores': precision_scores, 'recall_scores': recall_scores, 'f1_scores':recall_scores, 'specificity_scores': specificity_scores, 'auc_scores': auc_scores})

model_report.to_csv('model_report.csv')







